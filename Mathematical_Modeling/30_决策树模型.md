# 决策树模型

决策树模型就是长得像树的模型。

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240824185649889.png" alt="image-20240824185649889" style="zoom:67%;" />

## 常用术语

**结点/节点（_Node_）**：样本数据的一个集合。包括根结点（**_Roof_**）、内部结点、叶子结点（**_Leaf_**）；

**分支（_Split_）**：依据何种原则将一个结点分割；

**剪枝**：

**预剪枝**：在创建树的同时进行剪枝

**后剪枝**：树建好了再进行修剪

## 树模型的优势

**自动处理大量自变量**：树模型会在所有自变量中依次选出最重要的自变量对样本进行切分。（自变量可以重复使用）

**对数据没有正态独立方差齐这些要求**：应用范围更广。

> 在回归分析中，假设检验的三个关键特性是：
>
> - **正态性**：要求回归模型的残差（实际值与预测值之间的差异）应接近正态分布，以保证统计检验的有效性。
> - **独立性**：要求回归模型的残差之间彼此独立，没有自相关性，以避免遗漏变量或模型不适问题。
> - **方差齐性**：要求回归模型的残差具有恒定的方差，即残差的分布应在预测值的不同范围内保持一致，以避免影响回归系数的标准误差估计。

## 怎么种（构造）一棵树

所谓“构造”，就是**选择一个自变量，依据某个标准，来分割节点**。

### 信息熵

信息熵表示的是信息的不确定程度。信息越不确定，熵值越大。

**不确定性函数**（概率越大，不确定性越小）：

$$
f(P) = \log \frac{1}{p} = -\log p
$$

**信息熵就是不确定函数的统计平均值**（0~1 之间，类似回归模型的残差平方和，表示模型的拟合效果）：

$$
H(U) = E[-\log p_i] = - \sum_{i = 1}^n p_i \log p_i
$$

比如，**集合 A 中，有 5 个健康人，1 个精神分裂症。集合 B 中，**

$$
Entropy(t) = -\frac{1}{6}\log_2\frac{1}{6} - \frac{5}{6}\log_2\frac{5}{6} = 0.65 \\
Entropy(t) = -\frac{3}{6}\log_2 \frac{3}{6} - \frac{3}{6}\log_2\frac{3}{6} = 1
$$

**对于二分类数据，信息熵图像如下**：

$$
H(U) = -P\log P - (1 - P)\log(1 - P)
$$

![Draft.drawio](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/Draft.drawio.svg)

## 信息增益（ID3 算法）、信息增益率（C4.5 算法）、基尼指数（Cart 算法）

**信息增益**指的就是某个划分可以带来纯度的提高，信息熵的下降。

依据信息增益构建树时，没有考虑自变量数量（属性或特征）的多少，**_ID3_** 算法倾向于选择取值比较多的自变量（属性）。

**信息增益率** = 信息增益 / 属性熵。**_C4.5_** 算法

（以上两种用的不多了，现在主要用下面介绍的这种）

---

**_Cart_** 算法，_Classification and Regression Tree_，分类回归树。

*ID3*与*C4.5*可以生成二叉树和多叉树，但 **_Cart_ 只能二叉树，既可分类树，也可以做回归树**。

### 基尼系数

$p(C_k \mid t)$ 表示结点 $t$ 属于类别 $C_k$ 的概率，结点 $t$ 的基尼系数为 1 减去各类别 $C_k$ 概率平方和。

$$
GINI(t) = 1 - \sum_k[p(C_k \mid t)]^2
$$

基尼系数越小，说明样本之间的差异性小，不确定程度低。

把一个结点分为两个后，算他们归一化基尼系数的和：

$$
GINI(D, A) = \frac{D_1}{D}GINI(D_1) + \frac{D_2}{D}GINI(D_2)
$$

**从所有的可能划分中找出 _Gini_ 系数最小（_Gini_ 系数改变最大）的划分**

## Sklearn 实现

![image-20240824204404379](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240824204404379.png)

```python
# 导入必要的库和模块
from sklearn import datasets  # 用于加载内置数据集
from sklearn.model_selection import train_test_split  # 用于分割数据集
from sklearn.tree import DecisionTreeClassifier  # 用于决策树分类

# 加载鸢尾花数据集
iris = datasets.load_iris()
# 提取特征变量（X）和目标变量（y）
X = iris.data  # 特征数据，形状为 (150, 4)，包括四个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度
y = iris.target  # 目标数据，形状为 (150,)，代表三种鸢尾花的类别

# 将数据集分割成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# test_size=0.3 表示 30% 的数据用于测试，70% 的数据用于训练

# 创建决策树分类器的实例
DTC = DecisionTreeClassifier()
# 使用训练数据拟合模型
DTC.fit(X_train, y_train)

# 打印特征名称，用于参考
print(iris.feature_names)
# 输出特征的重要性，表示每个特征在决策树中的贡献程度
DTC.feature_importances_

# 评估模型在测试集上的准确率
DTC.score(X_test, y_test)

# 导入分类报告模块
from sklearn.metrics import classification_report
# 打印分类报告，包含精确率、召回率和 F1 分数等指标
print(classification_report(y_test, DTC.predict(X_test)))

```

## 看看树长什么样子

```python
from sklearn.tree import export_graphviz

export_graphviz(
    DTC,
    out_file='classify_tree.dot',
    feature_names=iris.feature_names,
    class_names=iris.target_names,
)
```

## 使用 graphviz 查看

```python
from sklearn.tree import export_graphviz

export_graphviz(
    DTC,
    out_file='classify_tree.dot',
    feature_names=iris.feature_names,
    class_names=iris.target_names,
)
```

输出相应的 `.dot` 文件后，可以利用在线工具转化为 `.pdf` 等形式。

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240824220102373.png" alt="image-20240824220102373" style="zoom:67%;" />

# 回归决策树

分类与回归的区别就是：**分类问题的因变量是分类变量，回归中的因变量是连续变量**

分类决策树中，用**信息熵**来表示结点的**混乱程度（不纯度）**

回归决策树中，改用**均方差**来表示混乱程度就好

分类决策树中，叶子结点中的**众树**就是输出结果

回归决策树中，该用叶子结点的**平均数**作为结果就好

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn import datasets
from sklearn.tree import export_graphviz

california = datasets.fetch_california_housing()
X = california.data
y = california.target

DTR = DecisionTreeRegressor(max_depth=3)

DTR.fit(X, y)

print(DTR.score(X, y))

export_graphviz(DTR, out_file='regress_tree.dot', feature_names=california.feature_names)
```

![image-20240825104829684](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240825104829684.png)
