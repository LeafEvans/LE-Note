# 聚类分析

1. **无监督问题**：我们手里没有标签
2. **聚类**：相似度东西分到一组
3. **难点**：如何评估，如何调参

## _Kmeans_ 算法

### 基本概念

- 为了得到簇的个数，需要指定 K 值（即数据被分为多少堆）
- **质心**：均值，即向量各维取平均即可
- **距离的度量**：常用欧几里得距离和余弦相似度（先标准化）
- **优化目标：**$\min\sum_{i = 1}^K\sum_{x \in C_i} dist(c_i, x)^2$（每个样本点到中心的距离越小越好）

### 工作流程

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240825162713526.png" alt="image-20240825162713526" style="zoom:50%;" />

### 优缺点

- 优势：

  简单、快速，适合常规数据集

- 劣势：
  - _K_ 值难确定
  - 复杂度与样本呈线性关系
  - 很难发现任意形态的簇
  - 初值对结果的影响很大，需要多次尝试

## _DBSCAN_ 算法

### 基本原理

- **基本概念**：_Density-Based Spatial Clustering of Applications with Noise_

- **核心对象**：若某个点的密度达到算法设定的阈值则称其为核心点。（即 $r$ 领域内点的数量不小于 $minPts$）
- **$\epsilon -$邻域的距离阈值**：设定的半径 $r$
- **直接密度可达**：若某点 $p$ 在点 $q$ 的 $r$ 邻域内，且 $q$ 是核心点则 $p-q$ 直接密度可达。
- **密度可达**：若有一个点的序列 $q_0,q_1,\dots,q_k$，对任意的 $q_i - q_{i - 1}$ 是直接密度可达的，则称从 $q_0$ 到 $q_k$ 密度可达，这实际上是<u>直接密度可达的“传播”</u>。
- **密度相连**：若从某核心点 $p$ 出发，点 $q$ 和点 $k$ 都是密度可达的，则称点 q 和点 k 是密度相连的。
- **边界点**：属于某一个类的非核心点，不能发展下线了。
- **噪声点**：不属于任何一个类簇的点，从任何一个核心点出发都是密度不可达的。

![image-20240825211353407](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240825211353407.png)

> 由离群点概念可以看出，_DBSCAN_ 算法可以进行异常检测。

### 工作流程

- **参数 D**：输入数据集
- **参数 $\epsilon$**：指定半径
- **$MinPts$**：密度阈值

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240825212335747.png" alt="image-20240825212335747" style="zoom:80%;" />

**参数选择**：

- 半径 $\epsilon$，可以根据 $K$ 距离来设定：找突变点；（距离大小发生突变的点，然后可以通过它来确定）

  $K$ 距离：给定数据集 $P = {p(i); i = 0, 1, \dots, n}$，计算点 $P(i)$ 到集合 $D$ 的子集 $S$ 中所有点之间的距离，距离按照从小到达的顺序排序，$d(k)$ 就被称为 $k -$距离。

- $MinPts$：$k-$距离中的$k$的值，一般取的小一些，多次尝试。

### 优缺点

**优势**：

- 不需要指定簇个数
- 可以发现任意形状的簇
- 擅长找到离群点（检测任务）
- 两个参数就够了

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240825214543257.png" alt="image-20240825214543257" style="zoom:50%;" />

**劣势**：

- 高维数据有些困难（可以做降维）
- 参数难以选择（参数对结果的影响非常大）
- `Sklearn` 中效率很慢（数据削减策略）

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/recording.gif" alt="recording" style="zoom:67%;" />

# 代码实现

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


class KMeans:
    def __init__(self, data, num_clustres):
        self.data = data
        self.num_clustres = num_clustres

    def train(self, max_iterations):
        centroids = KMeans.centroids_init(self.data, self.num_clustres)
        num_examples = self.data.shape[0]
        closest_centroids_ids = np.empty((num_examples, 1))

        for _ in range(max_iterations):
            closest_centroids_ids = KMeans.centroids_find_closest(self.data, centroids)
            centroids = KMeans.centroids_compute(
                self.data, closest_centroids_ids, self.num_clustres
            )
        return centroids, closest_centroids_ids

    @staticmethod
    def centroids_init(data, num_clustres):
        num_examples = data.shape[0]
        random_ids = np.random.permutation(num_examples)
        centroids = data[random_ids[:num_clustres], :]
        return centroids

    @staticmethod
    def centroids_find_closest(data, centroids):
        num_examples = data.shape[0]
        num_centroids = centroids.shape[0]
        closest_centroids_ids = np.zeros((num_examples, 1))
        for example_index in range(num_examples):
            distance = np.zeros((num_centroids, 1))
            for centroid_index in range(num_centroids):
                distance_diff = data[example_index, :] - centroids[centroid_index, :]
                distance[centroid_index] = np.sum(distance_diff**2)
            closest_centroids_ids[example_index] = np.argmin(distance)
        return closest_centroids_ids

    @staticmethod
    def centroids_compute(data, closest_centroids_ids, num_clustres):
        num_features = data.shape[1]
        centroids = np.zeros((num_clustres, num_features))
        for centroids_id in range(num_clustres):
            closest_ids = closest_centroids_ids == centroids_id
            centroids[centroids_id, :] = np.mean(data[closest_ids.flatten(), :], axis=0)
        return centroids
```

```python
data = pd.read_csv('../data/iris.csv')
iris_types = ['SETOSA', 'VERESCOLOR', 'VIRGINICA']

x_axis = 'petal_length'
y_axis = 'petal_width'

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)

for iris_type in iris_types:
    plt.scatter(
        data[x_axis][data['class'] == iris_type],
        data[y_axis][data['class'] == iris_type],
        label=iris_type,
    )
plt.title('Label Known')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(data[x_axis][:], data[y_axis][:])
plt.title('Label Unknown')
plt.show()
```

![image-20240826174903858](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240826174903858.png)

```python
num_examples = data.shape[0]
x_train = data[[x_axis, y_axis]].values

num_clustres = 3
max_iterations = 50

k_means = KMeans(x_train, num_clustres)
centroids, closest_centroids_ids = k_means.train(max_iterations)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)

for iris_type in iris_types:
    plt.scatter(
        data[x_axis][data['class'] == iris_type],
        data[y_axis][data['class'] == iris_type],
        label=iris_type,
    )
plt.title('Label Known')
plt.legend()

plt.subplot(1, 2, 2)
for centroid_id, centroid in enumerate(centroids):
    current_examples_index = (closest_centroids_ids == centroid_id).flatten()
    plt.scatter(
        x_train[current_examples_index, 0],
        x_train[current_examples_index, 1],
        label=centroid_id,
    )

for centroid_id, centroid in enumerate(centroids):
    plt.scatter(centroid[0], centroid[1], c='k', marker='*')

plt.title('Label Kmeans')

plt.show()
```

![image-20240826183631334](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240826183631334.png)

# 聚类算法实践

- _KMeans_ 与 _Dbscan_ 算法
- 半监督问题解决方案
- 聚类评估方法

```python
import numpy as np
import os
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
import warnings
warnings.filterwarnings('ignore')
np.random.seed(42)
```

## _KMeans_

```python
from sklearn.datasets import make_blobs

blob_centers = np.array(
    [[0.2, 2.3], [-1.5, 2.3], [-2.8, 1.8], [-2.8, 2.8], [-2.8, 1.3]]
)

blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])
```

```python
X, y = make_blobs(
    n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=7
)
```

```python
def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c='r', s=1)
    plt.xlabel('$x_1$', fontsize=14)
    plt.ylabel('$x_2$', fontsize=14, rotation=0)
plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.show()
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240826195606505.png" alt="image-20240826195606505" style="zoom:80%;" />

## 决策边界

```python
from sklearn.cluster import KMeans

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
```

```python
y_pred
```

```bash
array([2, 2, 4, ..., 1, 4, 2])
```

```python
kmeans.labels_
```

```bash
array([2, 2, 4, ..., 1, 4, 2])
```

```python
kmeans.cluster_centers_
```

```bash
array([[-0.066884  ,  2.10378803],
       [-2.79290307,  2.79641063],
       [-2.80214068,  1.55162671],
       [-1.47468607,  2.28399066],
       [ 0.47042841,  2.41380533]])
```

```python
X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)
```

```bash
array([0, 4, 1, 1])
```

```python
kmeans.transform(X_new)  # 代表每个点到簇的距离
```

```bash
array([[0.12347236, 2.9042344 , 2.83778629, 1.50178217, 0.62652832],
       [3.06863967, 5.84739223, 5.81943941, 4.48368889, 2.56319474],
       [3.06697984, 0.29040966, 1.4618254 , 1.685008  , 3.51958769],
       [2.95975563, 0.36159148, 0.96879317, 1.54053323, 3.47149865]])
```

---

```python
# 绘制数据点的函数
def plot_data(X):
    plt.scatter(X[:, 0], X[:, 1], color='k', s=2)  # 使用 plt.scatter 而不是 plt.plot


# 绘制质心的函数
def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        selected_centroids = centroids[weights > weights.max() / 10]
    else:
        selected_centroids = centroids

    plt.scatter(
        selected_centroids[:, 0],
        selected_centroids[:, 1],
        marker='o',
        s=30,
        linewidths=8,
        color=circle_color,
        zorder=10,
        alpha=0.9,
    )
    plt.scatter(
        selected_centroids[:, 0],
        selected_centroids[:, 1],
        marker='x',
        s=50,
        linewidths=2,  # 改为更合理的值
        color=cross_color,
        zorder=11,
        alpha=1,
    )


# 绘制决策边界的函数
def plot_decision_boundaries(
    clusterer,
    X,
    resolution=1000,
    show_centroids=True,
    show_xlabels=True,
    show_ylabels=True,
):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(
        np.linspace(mins[0], maxs[0], resolution),
        np.linspace(mins[1], maxs[1], resolution),
    )
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap='Pastel2')
    plt.contour(
        Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k'
    )

    plot_data(X)

    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel('$x_1$', fontsize=14)
    else:
        plt.tick_params(labelbottom='off')

    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')

```

```python
plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)
plt.show()
```

![image-20240826204452173](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240826204452173.png)

## 算法流程

```python
kmeans_iter1 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=1, random_state=1)
kmeans_iter2 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=2, random_state=1)
kmeans_iter3 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=3, random_state=1)

kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)
```

```python
plt.figure(figsize=(10, 8))
plt.subplot(321)
plot_data(X)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='k')
plt.title('Update Cluster Centers')

plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plt.title('Label')

plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter2.cluster_centers_)

plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)

plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter3.cluster_centers_)

plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, X, show_xlabels=False, show_ylabels=False)

plt.show()
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827095118278.png" alt="image-20240827095118278" style="zoom:80%;" />

## 不稳定的结果

```python
def plot_clusterer_comparison(c1, c2, X):
    c1.fit(X)
    c2.fit(X)

    plt.figure(figsize=(12, 4))
    plt.subplot(121)
    plot_decision_boundaries(c1, X)

    plt.subplot(122)
    plot_decision_boundaries(c2, X)
```

```python
c1 = KMeans(n_clusters=5, init='random', n_init=1, random_state=11)
c2 = KMeans(n_clusters=5, init='random', n_init=1, random_state=5)
plot_clusterer_comparison(c1, c2, X)
```

![image-20240827103715064](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827103715064.png)

## 评估指标

- _Inertia_ 指标：每个样本与其质心的距离

```python
kmeans.inertia_
```

```bash
224.0743312251573
```

```python
X_dist = kmeans.transform(X)
X_dist
```

```bash
array([[2.73844431, 1.45402521, 0.23085922, 1.54204522, 3.34391922],
       [2.82306816, 0.99002955, 0.26810747, 1.47805666, 3.39912897],
       [1.38822261, 4.09069201, 3.78216716, 2.67794429, 1.33653447],
       ...,
       [2.80678045, 0.06769209, 1.17785478, 1.40106167, 3.28864968],
       [0.42764894, 3.05913478, 3.15905017, 1.71924814, 0.23020951],
       [2.5754254 , 0.85434589, 0.43658314, 1.20868514, 3.1420062 ]])
```

```python
kmeans.labels_
```

```bash
array([2, 2, 4, ..., 1, 4, 2])
```

- _transform_ 得到的是当前样本到每个簇中心距离
- _n_init_ 的目标就是多次尝试得到最优 _Inertia_

```python
np.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_]**2)  # 结果与 inertia 一致
```

```python
kmeans.score(X)
```

```bash
-224.0743312251573
```

## 找到最佳簇数

如果 $k$ 值越大，得到的结果会越来越小。

```python
kmeans_per_k = [KMeans(n_clusters=k).fit(X) for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]
```

```python
plt.figure(figsize=(8, 4))
plt.plot(range(1, 10), inertias, 'o-')
plt.axis([1, 7.5, 0, 1300])
plt.show()
```

可以看到拐点为 4，因此可以选为 $k$ 值。（还要按照实际情况）

![image-20240827113348535](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827113348535.png)

## 轮廓系数

- $a_i$：计算样本 $i$ 到同簇其他样本的平均距离 $a_i$。$a_i$ 越小，说明样本 $i$ 越应该被聚类到该簇。将 $a_i$ 称为样本 $i$ 的簇内不相似度。
- $b_i$：计算样本 $i$ 到其他某簇 $C_j$ 的所有样本的平均距离 $b_{ij}$，称为样本 $i$ 与簇 $C_j$ 的不相似度。定义为样本 $i$ 的簇间不相似度：$b_i = \min\{b_{i1}, b_{i2}, \dots, b_{ik}\}$

$$
s(i) = \dfrac{b(i) - a(i)}{\max\{{a(i), b(i)}\}} \\

s(i) =
\begin{cases}
1 - \dfrac{a(i)}{b(i)}, &a(i) < b(i) \\
0, &a(i) = b(i) \\
\dfrac{b(i)}{a(i)} - 1, &a(i) > b(i)
\end{cases}
$$

**结论**：

- $s_i$ 接近 1，则说明样本 $i$ 聚类合理；
- $s_i$ 接近 -1，则说明样本 $i$ 更应该分类到另外的簇；
- 若 $s_i$ 近似为 0，则说明样本 $i$ 在两个簇的边界上。

---

```python
from sklearn.metrics import silhouette_score
silhouette_score(X, kmeans.labels_)
```

```python
silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]
silhouette_scores
```

```python
plt.figure(figsize=(8, 4))
plt.plot(range(2, 10), silhouette_scores, 'o-')
plt.show()
```

![image-20240827115815168](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827115815168.png)

## _KMeans_ 算法存在的问题

```python
X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 +[6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]

plot_data(X)
```

```python
plt.figure(figsize=(10, 4))
plt.subplot(121)
plot_decision_boundaries(kmeans_good, X)
plt.title(f'Good Inertia = {kmeans_good.inertia_}')

plt.subplot(122)
plot_decision_boundaries(kmeans_bad, X)
plt.title(f'Bad Inertia = {kmeans_bad.inertia_}')
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827135255532.png" alt="image-20240827135255532" style="zoom: 80%;" />

## 图像分割小例子

```python
from matplotlib.image import imread
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np

# 读取并查看图像
image = imread('./data/flower.jpg')
print(image.shape)  # 打印图像形状

# 展平图像数据
X = image.reshape(-1, 3)
print(X.shape)

# 创建一个 KMeans 模型并进行聚类
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
print(kmeans.cluster_centers_)

# 生成分割图像
segmented_imgs = []
n_colors = (10, 8, 6, 4, 2)
for n_clusters in n_colors:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    # 确保数据范围在 [0, 255] 或 [0, 1]
    segmented_img = np.clip(segmented_img, 0, 255)
    segmented_imgs.append(segmented_img.reshape(image.shape))

# 绘制原始图像和分割图像
plt.figure(figsize=(10, 5))
plt.subplot(231)
plt.imshow(image)
plt.title('Original Image')

for idx, n_clusters in enumerate(n_colors):
    plt.subplot(232 + idx)
    plt.imshow(segmented_imgs[idx].astype(np.uint8))  # 转换为 uint8 类型
    plt.title(f'{n_clusters} Colors')

plt.tight_layout()
plt.show()

```

![image-20240827143554019](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827143554019.png)

## 半监督学习

首先，将训练集聚类为 50 个集群，然后对于每个聚类，找到最靠近质心的图像。将这些图像称为代表性图像：

```python
from sklearn.datasets import load_digits

X_digits, y_digits = load_digits(return_X_y=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)
```

```python
X_train.shape
```

```python
from sklearn.linear_model import LogisticRegression

n_labeled = 50

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
log_reg.score(X_test, y_test)
```

```python
k = 50
kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)
X_digits_dist
```

```python
X_representative_digits_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits_idx.shape
```

```python
X_representative_digits = X_train[X_representative_digits_idx]
```

```python
plt.figure(figsize=(8, 2))

for index, X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10, 10, index + 1)
    plt.imshow(X_representative_digit.reshape(8, 8))
    plt.axis('off')

plt.show()
```

![image-20240827153249070](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827153249070.png)

```python
y_representative_digits = np.array([
    9, 8, 6, 0, 1, 7, 8, 9, 5, 1, 9, 6, 1, 0, 1, 2, 7, 1, 7, 8, 4, 5, 9, 8, 8,
    3, 5, 3, 2, 4, 8, 0, 7, 6, 2, 8, 3, 9, 0, 3, 1, 7, 2, 3, 4, 7, 1, 2, 8, 4
])
```

现在我们有一个只有 50 个标记示例的数据集，它们中的每一个都是其集群的代表性图像，而不是完全随机的实例。观察其性能是否更好。

```python
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)
```

更进一步：将标签传播到同一群集的所有其他实例。

```python
y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train_propagated)
```

只选择前 20 个：

```python
percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1
```

```python
partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]
```

```python
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
```

## _DBSCAN_

```python
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)
plt.plot(X[:, 0], X[:, 1], '.')
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827165704138.png" alt="image-20240827165704138" style="zoom:67%;" />

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```

```python
dbscan2 = DBSCAN(eps=0.2, min_samples=5)
dbscan2.fit(X)
```

```python
def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]

    plt.scatter(cores[:, 0],
                cores[:, 1],
                c=dbscan.labels_[core_mask],
                marker='o',
                s=size,
                cmap="Paired")
    plt.scatter(cores[:, 0],
                cores[:, 1],
                marker='*',
                s=20,
                c=dbscan.labels_[core_mask])
    plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', marker="x", s=100)
    plt.scatter(non_cores[:, 0],
                non_cores[:, 1],
                c=dbscan.labels_[non_core_mask],
                marker='.')
    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom='off')

    if show_ylabels:
        plt.ylabel('$x_2$', fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')
    plt.title('eps={:.2f}, min_samples={}'.format(dbscan.eps, dbscan.min_samples), fontsize=14)

```

```python
plt.figure(figsize=(9, 3.2))

plt.subplot(121)
plot_dbscan(dbscan, X, size=100)

plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)

plt.show()
```

![image-20240827171655558](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827171655558.png)

# 层次聚类

## 概念

层次聚类（_Hierarchical Clustering_）是聚类算法的一种，通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最底层，树的顶层是一个聚类的根结点。创建聚类树有自下而上合并和自上而下分裂两种方法。

作为一家公司的人力资源部经理，你可以将所有的雇员组织成较大的簇，如主管、经理和职员；然后你可以进一步划分为较小的簇，例如，职员簇可以进一步划分为子簇：高级职员、一般职员和实习人员。所有的这些簇形成了层次结构，可以很容易地对各层次 1 上的数据进行汇总或者特征化。

![image-20240827172355867](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827172355867.png)

## 划分方法

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827181732527.png" alt="image-20240827181732527" style="zoom:80%;" />

直观来看，上图中展示的数据划分为 2 个簇或 4 个簇都是合理的，甚至，如果上面每一个圈的内部包含的是大量数据形成的数据集，那么也许分成 16 个簇才是所需要的。

论数据集应该聚类成多少个族，通常是在讨论我们在件么尺度上关注这个类算法的优点之一是可以在不同的尺度上（层次）展示数据集的聚类情况。

基于层次的聚类算法（_Hierarchical Clustering_）可以是凝聚的（_Agglomerative_）或者分裂的（_Divisive_），取决于层次的划分是“自底向上”还是“自顶向下”。

## 自底向上的合并算法

层次算法的合并算法通过计算两类数据点间的相似性，对所有数据点中最为相似的两个数据点进行组合，并反复迭代这一过程。简单的说层次聚类的合并算法是通过计算每一个类别的数据点与所有数据点之间的距离来确定它们之间的相似性，距离越小，相似度越高。并将距离最近的两个数据点或类别进行组合，生成聚类树。

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827183155983.png" alt="image-20240827183155983" style="zoom:80%;" />

## 相似度的计算

层次聚类使用欧式距离来计算不同类别数据点的距离（相似度）。

$$
D = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}
$$

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827183548746.png" alt="image-20240827183548746" style="zoom:67%;" />

将数据点 _B_ 与数据点 _C_ 进行组合后，重新计算各类别数据点的距离矩阵。数据点间的距离计算公式与之前的方法一样。需要说明的是组合数据点 $(B, C)$ 与其他数据点间的计算方法。当我们计算 $(B, C)$ 到 $A$ 的距离时，需要分别计算 $B$ 到 $A$ 和 $C$ 到 $A$ 的距离均值。

$$
D = \dfrac{\sqrt{(B - A)^2} + \sqrt{(C - A)^2}}{2} = \dfrac{21.6 + 22.6}{2}
$$

经过计算数据点 $D$ 到数据点 $E$ 的距离在所有的距离值中最小，为 $1.20$ 。这表示在当前的所有数据点中（包含组合数据点），$D$ 和 $E$ 的相似度最高。因此我们将数据点 $D$ 和数据点 $E$ 进行组合。并在次计算其他数据点间的距离。

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827184546270.png" alt="image-20240827184546270" style="zoom:80%;" />

后面的工作就是不断的重复计算数据点与数据点，数据点与组合数据点间的距离。这个步骤应该由程序来完成。这里由于数据量较小，手工计算并列出每一步的距离计算和数据点组合的效果。

## 两个组合数据点间的距离

计算两个组合数据点距离的方法有三种，分别为 _Single Linkage_，_Complete Linkage_ 和 _Average Linkage_。

- _Single Linkage_：方法是将两个组合数据点中距离较近的两个数据点间的距离作为这两个组合数据点的距离。这种方法易受极端值的影响。两个很相似的组合数据点可能由于其中的某个极端数据点距离较近而组合在一起。
- _Complete Linkage_：_Complete Linkage_ 的计算方法与 _Single Linkage_ 相反，将两个组合数据点中距离最远的两个数据点的距离作为这两个数据点的距离。_Complete Linkage_ 的问题也与 _Single Linkage_ 相反，两个不相似的组合数据点可能由于其中的极端值距离较远而无法组合在一起。
- _Average Linkage_：_Average Linkage_ 的计算方法是计算两个组合数据点中的每个数据点与其他所有数据点的距离。将所有距离的均值作为两个组合数据点间的距离。这种方法计算量比较大，但结果比前两种更加合理。

使用 _Average Linkage_ 计算组合数据点间的距离。下面是计算组合数据点 $(A, F)$ 到 $(B, C)$ 的距离，分别计算了 $(A,F)$ 和 $(B,C)$ 两两间距离的均值。

$$
D = \dfrac{\sqrt{(A - B)^2} + \sqrt{(A - C)^2} + \sqrt{(F - B)^2} + \sqrt{(F - C)^2}}{4}
$$

## 树状图

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827201010875.png" alt="image-20240827201010875" style="zoom:67%;" />

## 层次聚类实例

```python
import pandas as pd

seeds_df = pd.read_csv('./datasets/seeds-less-rows.csv')
seefs_df.head()
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827201856807.png" alt="image-20240827201856807" style="zoom:80%;" />

```python
seeds_df.grain_varity.value_counts()
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827202421677.png" alt="image-20240827202421677" style="zoom:67%;" />

```python
varieties = list(seeds_df.pop('grain_variety'))

samples = seeds_df.values
```

```python
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt
```

```python
# 进行层次聚类
mergings = linkage(samples, method='complete')
```

```python
# 树状图结果
fig = plt.figure(figsize=(10, 6))
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=6,
       		 )
plt.show()
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827203122430.png" alt="image-20240827203122430" style="zoom:50%;" />

```python
# 得到标签结果
# maximum height 自己指定
from scipy.cluster.hierarchy import fcluster
labels = fcluster(mergings, 6, criterion='distance')

df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct = pd.crosstab(df['labels'], df['varieties'])
ct
```

![image-20240827204443443](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827204443443.png)

**不同距离的选择会产生不同的结果**

```python
import pandas as pd

scores_df = pd.read_csv('./datasets/eurovision.csv', index_col=0)
country_names = list(scores_df.index)
scores_df.head()
```

![image-20240827210250735](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827210250735.png)

```python
# 缺失值填充，没有的就先按满分计算
scores_df = scores_df.fillna(12)
```

```python
from sklearn.preprocessing import normalize
samples = normalize(scores_df.values)
```

> 问题中涉及到距离时，要考虑进行归一化处理。

```python
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

mergings = linkage(samples, method='single')
fig = plt.figure(figsize=(10, 6))
dendrogram(mergings,
           labels=country_names,
           leaf_rotation=90,
           leaf_font_size=6,
          )
plt.show()
```

![image-20240827211231060](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827211231060.png)

```python
mergings = linkage(samples, method='complete')
fig = plt.figure(figsize=(10, 6))
dendrogram(mergings,
           labels=country_names,
           leaf_rotation=90,
           leaf_font_size=6,
          )
plt.show()
```

![image-20240827211314177](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827211314177.png)

# 聚类算法实战

## _KMeans_ 算法

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240827211411732.png" alt="image-20240827211411732" style="zoom:80%;" />

---

```python
import pandas as pd

beer = pd.read_csv('./data/data.csv')
beer
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Budweiser</td>
      <td>144</td>
      <td>15</td>
      <td>4.7</td>
      <td>0.43</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Schlitz</td>
      <td>151</td>
      <td>19</td>
      <td>4.9</td>
      <td>0.43</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lowenbrau</td>
      <td>157</td>
      <td>15</td>
      <td>0.9</td>
      <td>0.48</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kronenbourg</td>
      <td>170</td>
      <td>7</td>
      <td>5.2</td>
      <td>0.73</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Heineken</td>
      <td>152</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.77</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Old_Milwaukee</td>
      <td>145</td>
      <td>23</td>
      <td>4.6</td>
      <td>0.28</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Miller</td>
      <td>150</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.50</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Coors</td>
      <td>148</td>
      <td>12</td>
      <td>4.5</td>
      <td>0.49</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Stella_Artois</td>
      <td>175</td>
      <td>18</td>
      <td>5.3</td>
      <td>0.85</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Guinness</td>
      <td>210</td>
      <td>22</td>
      <td>4.2</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Corona</td>
      <td>139</td>
      <td>10</td>
      <td>4.6</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Blue_Moon</td>
      <td>180</td>
      <td>19</td>
      <td>5.4</td>
      <td>0.78</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Beck's</td>
      <td>142</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.65</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Amstel</td>
      <td>146</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.60</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Sam_Adams</td>
      <td>180</td>
      <td>13</td>
      <td>5.1</td>
      <td>0.85</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Pilsner_Urquell</td>
      <td>170</td>
      <td>10</td>
      <td>4.9</td>
      <td>0.75</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Carlsberg</td>
      <td>155</td>
      <td>12</td>
      <td>4.6</td>
      <td>0.55</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Hoegaarden</td>
      <td>165</td>
      <td>15</td>
      <td>4.9</td>
      <td>0.68</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Peroni</td>
      <td>140</td>
      <td>9</td>
      <td>4.7</td>
      <td>0.58</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Sapporo</td>
      <td>148</td>
      <td>13</td>
      <td>5.2</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Asahi</td>
      <td>150</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.72</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Leffe</td>
      <td>158</td>
      <td>14</td>
      <td>6.5</td>
      <td>0.80</td>
    </tr>
  </tbody>
</table>
```python
X = beer[['calories', 'sodium', 'alcohol', 'cost']]
```

```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3).fit(X)
km2 = KMeans(n_clusters=2).fit(X)
```

```python
km.labels_
```

```bash
array([0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0])
```

```python
beer['cluster'] = km.labels_
beer['cluster2'] = km2.labels_
beer.sort_values('cluster')
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster</th>
      <th>cluster2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Budweiser</td>
      <td>144</td>
      <td>15</td>
      <td>4.7</td>
      <td>0.43</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Sapporo</td>
      <td>148</td>
      <td>13</td>
      <td>5.2</td>
      <td>0.70</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Peroni</td>
      <td>140</td>
      <td>9</td>
      <td>4.7</td>
      <td>0.58</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Carlsberg</td>
      <td>155</td>
      <td>12</td>
      <td>4.6</td>
      <td>0.55</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Amstel</td>
      <td>146</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.60</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Beck's</td>
      <td>142</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.65</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Asahi</td>
      <td>150</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.72</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Corona</td>
      <td>139</td>
      <td>10</td>
      <td>4.6</td>
      <td>0.70</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Miller</td>
      <td>150</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.50</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Old_Milwaukee</td>
      <td>145</td>
      <td>23</td>
      <td>4.6</td>
      <td>0.28</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Heineken</td>
      <td>152</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.77</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lowenbrau</td>
      <td>157</td>
      <td>15</td>
      <td>0.9</td>
      <td>0.48</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Schlitz</td>
      <td>151</td>
      <td>19</td>
      <td>4.9</td>
      <td>0.43</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Coors</td>
      <td>148</td>
      <td>12</td>
      <td>4.5</td>
      <td>0.49</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Leffe</td>
      <td>158</td>
      <td>14</td>
      <td>6.5</td>
      <td>0.80</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Stella_Artois</td>
      <td>175</td>
      <td>18</td>
      <td>5.3</td>
      <td>0.85</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Blue_Moon</td>
      <td>180</td>
      <td>19</td>
      <td>5.4</td>
      <td>0.78</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Sam_Adams</td>
      <td>180</td>
      <td>13</td>
      <td>5.1</td>
      <td>0.85</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Pilsner_Urquell</td>
      <td>170</td>
      <td>10</td>
      <td>4.9</td>
      <td>0.75</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kronenbourg</td>
      <td>170</td>
      <td>7</td>
      <td>5.2</td>
      <td>0.73</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Hoegaarden</td>
      <td>165</td>
      <td>15</td>
      <td>4.9</td>
      <td>0.68</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Guinness</td>
      <td>210</td>
      <td>22</td>
      <td>4.2</td>
      <td>0.95</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
```python
from pandas.plotting import scatter_matrix
%matplotlib inline

cluster*centers = km.cluster_centers*

cluster*centers_2 = km2.cluster_centers*

````

```python
beer.groupby('cluster').mean(numeric_only=True)
````

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster2</th>
    </tr>
    <tr>
      <th>cluster</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>148.333333</td>
      <td>13.533333</td>
      <td>4.653333</td>
      <td>0.578667</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>173.333333</td>
      <td>13.666667</td>
      <td>5.133333</td>
      <td>0.773333</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>210.000000</td>
      <td>22.000000</td>
      <td>4.200000</td>
      <td>0.950000</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>

```python
beer.groupby('cluster2').mean(numeric_only=True)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster</th>
    </tr>
    <tr>
      <th>cluster2</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>155.47619</td>
      <td>13.571429</td>
      <td>4.790476</td>
      <td>0.634286</td>
      <td>0.285714</td>
    </tr>
    <tr>
      <th>1</th>
      <td>210.00000</td>
      <td>22.000000</td>
      <td>4.200000</td>
      <td>0.950000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>

```python
centers = beer.groupby('cluster').mean(numeric_only=True).reset_index()
centers
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cluster</th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>148.333333</td>
      <td>13.533333</td>
      <td>4.653333</td>
      <td>0.578667</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>173.333333</td>
      <td>13.666667</td>
      <td>5.133333</td>
      <td>0.773333</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>210.000000</td>
      <td>22.000000</td>
      <td>4.200000</td>
      <td>0.950000</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>

```python
import matplotlib.pyplot as plt
plt.rcParams['font.size'] = 14
import numpy as np
colors = np.array(['red', 'green', 'blue', 'yellow'])

plt.scatter(beer['calories'], beer['alcohol'], c=colors[beer['cluster']])
plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black')

plt.xlabel('Calories')
plt.ylabel('Alcohol')
```

![image-20240828121747052](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240828121747052.png)

```python
scatter_matrix(beer[['calories', 'sodium', 'alcohol', 'cost']], s=100, alpha=1,
               c=colors[beer['cluster']], figsize=(10, 10))
plt.suptitle('With 3 centroids initialized')
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240828122326366.png" alt="image-20240828122326366" style="zoom:67%;" />

```python
scatter_matrix(beer[['calories', 'sodium', 'alcohol', 'cost']], s=100, alpha=1,
               c=colors[beer['cluster2']], figsize=(10, 10))
plt.suptitle('With 2 centroids initialized')
```

<img src="https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240828122529606.png" alt="image-20240828122529606" style="zoom:67%;" />

---

**Scaled Data**

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled
```

```bash
array([[-8.33203070e-01,  2.59543429e-01, -6.70263212e-02,
        -1.34763504e+00],
       [-4.15244527e-01,  1.25257916e+00,  1.43627831e-01,
        -1.34763504e+00],
       [-5.69943468e-02,  2.59543429e-01, -4.06945522e+00,
        -1.03944407e+00],
       [ 7.19214376e-01, -1.72652802e+00,  4.59609060e-01,
         5.01510752e-01],
       [-3.55536163e-01, -7.33492298e-01,  2.48954907e-01,
         7.48063524e-01],
       [-7.73494706e-01,  2.24561488e+00, -1.72353397e-01,
        -2.27220793e+00],
       [-4.74952890e-01,  1.12844969e-02,  3.83007550e-02,
        -9.16167687e-01],
       [-5.94369617e-01, -4.85233366e-01, -2.77680474e-01,
        -9.77805880e-01],
       [ 1.01775619e+00,  1.00432022e+00,  5.64936136e-01,
         1.24116907e+00],
       [ 3.10754891e+00,  1.99735595e+00, -5.93661702e-01,
         1.85755100e+00],
       [-1.13174489e+00, -9.81751230e-01, -1.72353397e-01,
         3.16596173e-01],
       [ 1.31629801e+00,  1.25257916e+00,  6.70263212e-01,
         8.09701717e-01],
       [-9.52619796e-01, -7.33492298e-01,  2.48954907e-01,
         8.40520814e-03],
       [-7.13786343e-01,  1.12844969e-02,  3.83007550e-02,
        -2.99785757e-01],
       [ 1.31629801e+00, -2.36974435e-01,  3.54281984e-01,
         1.24116907e+00],
       [ 7.19214376e-01, -9.81751230e-01,  1.43627831e-01,
         6.24787138e-01],
       [-1.76411073e-01, -4.85233366e-01, -1.72353397e-01,
        -6.07976722e-01],
       [ 4.20672560e-01,  2.59543429e-01,  1.43627831e-01,
         1.93319787e-01],
       [-1.07203652e+00, -1.23001016e+00, -6.70263212e-02,
        -4.23062143e-01],
       [-5.94369617e-01, -2.36974435e-01,  4.59609060e-01,
         3.16596173e-01],
       [-4.74952890e-01, -7.33492298e-01,  2.48954907e-01,
         4.39872559e-01],
       [ 2.71401651e-03,  1.12844969e-02,  1.82886105e+00,
         9.32978103e-01]])
```

```python
km3 = KMeans(n_clusters=3).fit(X_scaled)
```

```python
beer['scaled_cluster'] = km3.labels_
beer.sort_values('scaled_cluster')
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster</th>
      <th>cluster2</th>
      <th>scaled_cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10</th>
      <td>Corona</td>
      <td>139</td>
      <td>10</td>
      <td>4.6</td>
      <td>0.70</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Sapporo</td>
      <td>148</td>
      <td>13</td>
      <td>5.2</td>
      <td>0.70</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Peroni</td>
      <td>140</td>
      <td>9</td>
      <td>4.7</td>
      <td>0.58</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Hoegaarden</td>
      <td>165</td>
      <td>15</td>
      <td>4.9</td>
      <td>0.68</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Pilsner_Urquell</td>
      <td>170</td>
      <td>10</td>
      <td>4.9</td>
      <td>0.75</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Beck's</td>
      <td>142</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.65</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Asahi</td>
      <td>150</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.72</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Leffe</td>
      <td>158</td>
      <td>14</td>
      <td>6.5</td>
      <td>0.80</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Heineken</td>
      <td>152</td>
      <td>11</td>
      <td>5.0</td>
      <td>0.77</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kronenbourg</td>
      <td>170</td>
      <td>7</td>
      <td>5.2</td>
      <td>0.73</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Miller</td>
      <td>150</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.50</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Old_Milwaukee</td>
      <td>145</td>
      <td>23</td>
      <td>4.6</td>
      <td>0.28</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Amstel</td>
      <td>146</td>
      <td>14</td>
      <td>4.8</td>
      <td>0.60</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Carlsberg</td>
      <td>155</td>
      <td>12</td>
      <td>4.6</td>
      <td>0.55</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Lowenbrau</td>
      <td>157</td>
      <td>15</td>
      <td>0.9</td>
      <td>0.48</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Schlitz</td>
      <td>151</td>
      <td>19</td>
      <td>4.9</td>
      <td>0.43</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Coors</td>
      <td>148</td>
      <td>12</td>
      <td>4.5</td>
      <td>0.49</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Budweiser</td>
      <td>144</td>
      <td>15</td>
      <td>4.7</td>
      <td>0.43</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Stella_Artois</td>
      <td>175</td>
      <td>18</td>
      <td>5.3</td>
      <td>0.85</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Guinness</td>
      <td>210</td>
      <td>22</td>
      <td>4.2</td>
      <td>0.95</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Blue_Moon</td>
      <td>180</td>
      <td>19</td>
      <td>5.4</td>
      <td>0.78</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Sam_Adams</td>
      <td>180</td>
      <td>13</td>
      <td>5.1</td>
      <td>0.85</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
```python
beer.groupby('scaled_cluster').mean(numeric_only=True)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>calories</th>
      <th>sodium</th>
      <th>alcohol</th>
      <th>cost</th>
      <th>cluster</th>
      <th>cluster2</th>
    </tr>
    <tr>
      <th>scaled_cluster</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>153.40</td>
      <td>11.1</td>
      <td>5.100</td>
      <td>0.7080</td>
      <td>0.30</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>149.50</td>
      <td>15.5</td>
      <td>4.225</td>
      <td>0.4700</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>186.25</td>
      <td>18.0</td>
      <td>5.000</td>
      <td>0.8575</td>
      <td>1.25</td>
      <td>0.25</td>
    </tr>
  </tbody>
</table>
```python
scatter_matrix(
    X,
    c=colors[beer.scaled_cluster],
    alpha=1,
    figsize=(10, 10),
    s=300,
)
```

![image-20240828130611877](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240828130611877.png)

```python
from sklearn import metrics

score_scaled = metrics.silhouette_score(X, beer.scaled_cluster)
score = metrics.silhouette_score(X, beer.cluster)
print(score_scaled, score)
```

$0.10366159902444833 0.5929341487589036$

```python
scores = []

for k in range(2, 20):
    labels = KMeans(n_clusters=k).fit(X).labels_
    score = metrics.silhouette_score(X, labels)
    scores.append(score)

scores
```

```bash
[0.6040960319606762,
 0.5929341487589036,
 0.37968070249950236,
 0.3672278477752512,
 0.36928250359135434,
 0.4019291611718076,
 0.32090562218825835,
 0.38734168261361474,
 0.34180752319870145,
 0.30326330982283356,
 0.27573937129039733,
 0.2605415698594175,
 0.19113241545383516,
 0.25340994510798304,
 0.19640063091923504,
 0.20669457105517494,
 0.14968525686642692,
 0.12129725273584376]
```

```python
plt.plot(list(range(2, 20)), scores)
plt.xlabel('Number of Clusters Initialized')
plt.ylabel('Sihouette Score')
```

![image-20240828132914700](https://leafalice-image.oss-cn-hangzhou.aliyuncs.com/img/image-20240828132914700.png)

其他一致。

# R 型聚类

**R 型聚类**（R-type clustering）是一种聚类分析方法，专门用于在数据集的变量（特征）之间寻找相似性或关联。与之对应的 **Q 型聚类**（Q-type clustering）是针对样本（对象）之间的相似性进行聚类分析。

## R 型聚类的目的

R 型聚类的主要目的是通过分析变量之间的相关性或相似性，将具有相似模式的变量聚合在一起。这种方法常用于数据维度较高的场合，通过聚类来减少特征维度，进而选出代表性的特征进行进一步分析或建模。

## R 型聚类的过程

R 型聚类是一种针对变量（特征）进行的聚类分析方法，通常用于识别和归类变量之间的相似性。它在化学、环境科学、社会科学等领域中应用广泛，特别是当研究者希望简化数据、减少维度时。

### 计算变量之间的相似度

- **目标**：首先，通过计算变量（特征）之间的相似度，确定哪些变量具有类似的行为或关系。
- **方法**：
  - **相关系数**：这是最常用的相似性度量方式，特别是在处理连续型数据时。相关系数（例如皮尔森相关系数）测量了两个变量之间的线性关系，其值范围从 -1 到 1。值接近 1 表示强正相关，接近 -1 表示强负相关，而接近 0 则表示无线性关系。
  - **欧氏距离**：对于多维数据，欧氏距离是另一种常用的度量方式。它计算了两点之间的直线距离，通常用于聚类分析中的距离度量。

### 构建相似性矩阵

- **目标**：基于相似性度量，构建一个相似性矩阵。这个矩阵的每个元素表示两个变量之间的相似度。
- **过程**：
  - **构建矩阵**：假设你有 n 个变量，你会得到一个 n x n 的矩阵，每个矩阵元素 \(r\_{ij}\) 代表第 i 个变量与第 j 个变量之间的相似性（例如，皮尔森相关系数）。
  - **对角元素**：通常，矩阵的对角线元素 \(r\_{ii}\) 是 1，因为变量与自身的相似度为最大。

### 聚类算法

- **目标**：对相似性矩阵进行聚类分析，将具有较高相似度的变量归为同一类。
- **方法**：
  - **层次聚类**：这是一种常用的聚类方法，特别是当研究者希望通过树状图（dendrogram）观察聚类结果时。层次聚类分为凝聚型和分裂型，常用的是凝聚型层次聚类，开始时每个变量各自为一类，然后逐渐合并相似的类。
  - **K-means 聚类**：这是另一种常用的聚类方法，它将数据分为预先确定的 k 个类，每个类代表一种聚类中心，算法通过迭代不断调整这些中心的位置，直到得到稳定的聚类结果。
  - **结果解释**：通过这些聚类算法，变量被分为多个类，具有较高相似性的变量会被归到同一个类中。最终的输出通常是一棵树状图或者一个分类表格。

### 解释聚类结果

- **目标**：识别和解释聚类结果，选出每个聚类中最具代表性的变量进行进一步分析。
- **方法**：
  - **分析聚类树**：树状图（dendrogram）是层次聚类的一种可视化方法，它展示了变量之间的聚类过程。研究者可以通过观察树状图，选择一个合理的聚类水平来确定变量的分类。
  - **选择代表性变量**：在每个聚类中，通常会选择一个或多个最具代表性的变量，这些变量是该类中最能反映整体特性的变量。这可以帮助简化后续的分析，减少维度，并提高模型的解释性。

通过这个过程，R 型聚类可以帮助研究者简化数据集，识别出关键变量，并更好地理解数据中的结构和模式。

## R 型聚类的应用场景

- **特征选择**：在高维数据中，R 型聚类可以帮助识别相关性高的变量，进而通过选择少数代表性变量来简化模型，避免冗余信息。
- **因子分析**：R 型聚类有助于在变量之间寻找潜在的因子结构，通过聚类结果进行因子旋转，找到潜在的共性因子。
- **数据降维**：通过聚类将高度相关的变量合并，减少模型的输入变量数量，提升模型的稳定性和解释性。

## 6.4 总结

R 型聚类是一种针对变量（特征）的聚类方法，用于通过分析变量之间的相似性或相关性，识别出具有相似特征的变量，并将其归类。它常用于特征选择、因子分析和数据降维等场景，有助于简化数据分析过程，提升分析效率。
